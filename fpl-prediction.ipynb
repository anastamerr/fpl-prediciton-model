{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Fantasy Premier League (FPL) Player Performance Prediction\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook implements a machine learning pipeline to predict player points for upcoming gameweeks in Fantasy Premier League using historical data and statistical analysis.\n",
    "\n",
    "### Objectives:\n",
    "1. **Data Cleaning**: Remove unnecessary columns and handle inconsistencies\n",
    "2. **Feature Engineering**: Create 'form' feature and analytical insights\n",
    "3. **Predictive Modeling**: Build regression model for upcoming_total_points\n",
    "4. **Model Explainability**: Implement SHAP and LIME for interpretability\n",
    "5. **Inference Function**: Create callable prediction function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import_libs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Model Explainability\n",
    "import shap\n",
    "from lime import lime_tabular\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_data",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# df = pd.read_csv(\"../dataset/cleaned_merged_seasons.csv\")\n",
    "# For local: df = pd.read_csv('../dataset/cleaned_merged_seasons.csv')\n",
    "df = pd.read_csv('/kaggle/input/dataset/cleaned_merged_seasons.csv')\n",
    "\n",
    "print(f'Dataset Shape: {df.shape}')\n",
    "print(f'\\nColumns: {df.columns.tolist()}')\n",
    "print(f'\\nFirst 5 rows:')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_info",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print('Dataset Info:')\n",
    "print(df.info())\n",
    "print('\\n' + '='*50)\n",
    "print('\\nMissing Values:')\n",
    "print(df.isnull().sum())\n",
    "print('\\n' + '='*50)\n",
    "print('\\nBasic Statistics:')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleaning_header",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning\n",
    "\n",
    "### Steps:\n",
    "- Remove columns related to player popularity (out of scope)\n",
    "- Handle missing values\n",
    "- Remove duplicates\n",
    "- Rename columns for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for cleaning\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Rename columns for consistency\n",
    "df_clean = df_clean.rename(columns={\n",
    "    'season_x': 'season',\n",
    "    'name': 'player_name',\n",
    "    'team_x': 'team',\n",
    "    'GW': 'gameweek'\n",
    "})\n",
    "\n",
    "# Remove popularity-related columns (out of scope)\n",
    "popularity_cols = ['selected', 'transfers_in', 'transfers_out', 'transfers_balance']\n",
    "df_clean = df_clean.drop(columns=[col for col in popularity_cols if col in df_clean.columns], errors='ignore')\n",
    "\n",
    "# Remove unnecessary columns\n",
    "unnecessary_cols = ['element', 'fixture', 'round', 'kickoff_time', 'opponent_team', \n",
    "                    'opp_team_name', 'team_a_score', 'team_h_score', 'was_home']\n",
    "df_clean = df_clean.drop(columns=[col for col in unnecessary_cols if col in df_clean.columns], errors='ignore')\n",
    "\n",
    "# Handle missing values\n",
    "print('Missing values before cleaning:')\n",
    "print(df_clean.isnull().sum().sum())\n",
    "\n",
    "# Fill numeric missing values with 0\n",
    "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "df_clean[numeric_cols] = df_clean[numeric_cols].fillna(0)\n",
    "\n",
    "# Remove duplicates\n",
    "print(f'\\nDuplicates before: {df_clean.duplicated().sum()}')\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "print(f'Duplicates after: {df_clean.duplicated().sum()}')\n",
    "\n",
    "# Sort by player and gameweek\n",
    "df_clean = df_clean.sort_values(['season', 'player_name', 'gameweek']).reset_index(drop=True)\n",
    "\n",
    "print(f'\\nCleaned dataset shape: {df_clean.shape}')\n",
    "print(f'\\nRemaining columns: {df_clean.columns.tolist()}')\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleaning_justification",
   "metadata": {},
   "source": [
    "### Data Cleaning Rationale\n",
    "\n",
    "**Why we removed these columns:**\n",
    "- **Popularity metrics** (selected, transfers_in/out): Reflect manager behavior, not player performance. Including these would create data leakage as they're influenced by future expectations.\n",
    "- **Match identifiers** (element, fixture, round): Administrative data with no predictive value.\n",
    "- **Temporal data** (kickoff_time): Time of day doesn't significantly impact FPL points.\n",
    "- **Match context** (opponent_team, was_home, scores): While potentially useful, these add complexity and our focus is on player-level performance patterns.\n",
    "\n",
    "**Missing value strategy:**\n",
    "- Numeric columns filled with 0: Represents \"no contribution\" (e.g., 0 goals, 0 assists)\n",
    "- This is appropriate for FPL data where absence of an event means zero points for that category\n",
    "\n",
    "**Why we sort by season, player, and gameweek:**\n",
    "- Enables time-series operations (rolling averages for form feature)\n",
    "- Ensures chronological order for creating lagged target variable\n",
    "- Prevents data leakage by maintaining temporal sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_eng_header",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "### Create 'form' Feature\n",
    "Form = Average total points over the past 4 gameweeks (if available) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_form",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create form feature\n",
    "def calculate_form(group):\n",
    "    group['form'] = group['total_points'].rolling(window=4, min_periods=1).mean() / 10\n",
    "    return group\n",
    "\n",
    "df_clean = df_clean.groupby(['season', 'player_name'], group_keys=False).apply(calculate_form)\n",
    "\n",
    "print('Form feature created successfully!')\n",
    "print(f'\\nForm statistics:')\n",
    "print(df_clean['form'].describe())\n",
    "print(f'\\nSample data with form:')\n",
    "df_clean[['season', 'player_name', 'gameweek', 'total_points', 'form']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_cleaned_dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataset with form column\n",
    "df_clean.to_csv(\"cleaned_merged_seasons_with_form.csv\", index=False)\n",
    "\n",
    "print(\"Cleaned dataset with form column saved successfully!\")\n",
    "print(f\"File: cleaned_merged_seasons_with_form.csv\")\n",
    "print(f\"Shape: {df_clean.shape}\")\n",
    "print(f\"Columns: {df_clean.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis_header",
   "metadata": {},
   "source": [
    "## 5. Data Analysis\n",
    "\n",
    "### Question 1: Position Analysis\n",
    "Which player positions score the largest sum of total points on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "position_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average total points by position\n",
    "position_stats = df_clean.groupby('position').agg({\n",
    "    'total_points': ['sum', 'mean', 'count']\n",
    "}).round(2)\n",
    "\n",
    "position_stats.columns = ['Total Points Sum', 'Average Points', 'Number of Records']\n",
    "position_stats = position_stats.sort_values('Average Points', ascending=False)\n",
    "\n",
    "print('Position Analysis - Total Points Statistics:')\n",
    "print(position_stats)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Bar plot\n",
    "axes[0].bar(position_stats.index, position_stats['Average Points'], \n",
    "            color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A'])\n",
    "axes[0].set_title('Average Total Points by Position', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Position', fontsize=12)\n",
    "axes[0].set_ylabel('Average Total Points', fontsize=12)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "df_clean.boxplot(column='total_points', by='position', ax=axes[1])\n",
    "axes[1].set_title('Distribution of Total Points by Position', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Position', fontsize=12)\n",
    "axes[1].set_ylabel('Total Points', fontsize=12)\n",
    "plt.suptitle('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nAnswer: {position_stats.index[0]} position scores highest average total points')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evolution_header",
   "metadata": {},
   "source": [
    "### Question 2: Performance Evolution\n",
    "Analyze top 5 players in 2022-23 season using the 'form' feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "top_players_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter 2022-23 season\n",
    "season_2022 = df_clean[df_clean['season'] == '2022-23'].copy()\n",
    "\n",
    "# Find top 5 players by total points\n",
    "top_players_total = season_2022.groupby('player_name')['total_points'].sum().nlargest(5)\n",
    "print('Top 5 Players by Total Points in 2022-23:')\n",
    "print(top_players_total)\n",
    "\n",
    "# Find top 5 players by average form\n",
    "top_players_form = season_2022.groupby('player_name')['form'].mean().nlargest(5)\n",
    "print('\\nTop 5 Players by Average Form in 2022-23:')\n",
    "print(top_players_form)\n",
    "\n",
    "# Compare\n",
    "print('\\n' + '='*60)\n",
    "print('Comparison: Top by Total Points vs Top by Form')\n",
    "print('='*60)\n",
    "comparison = pd.DataFrame({\n",
    "    'Top by Total Points': top_players_total.index.tolist(),\n",
    "    'Top by Form': top_players_form.index.tolist()\n",
    "})\n",
    "print(comparison)\n",
    "\n",
    "# Visualize evolution\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# Plot 1: Form evolution\n",
    "for player in top_players_total.index:\n",
    "    player_data = season_2022[season_2022['player_name'] == player]\n",
    "    axes[0].plot(player_data['gameweek'], player_data['form'], marker='o', label=player, linewidth=2)\n",
    "\n",
    "axes[0].set_title('Form Evolution of Top 5 Players (2022-23)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Gameweek', fontsize=12)\n",
    "axes[0].set_ylabel('Form (Avg Points / 10)', fontsize=12)\n",
    "axes[0].legend(loc='best', fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Total points per gameweek\n",
    "for player in top_players_total.index:\n",
    "    player_data = season_2022[season_2022['player_name'] == player]\n",
    "    axes[1].plot(player_data['gameweek'], player_data['total_points'], \n",
    "                marker='s', label=player, linewidth=2, alpha=0.7)\n",
    "\n",
    "axes[1].set_title('Total Points per Gameweek - Top 5 Players (2022-23)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Gameweek', fontsize=12)\n",
    "axes[1].set_ylabel('Total Points', fontsize=12)\n",
    "axes[1].legend(loc='best', fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Answer\n",
    "overlap = set(top_players_total.index) & set(top_players_form.index)\n",
    "print(f'\\nAnswer: {len(overlap)} out of 5 top players appear in both lists.')\n",
    "if overlap:\n",
    "    players_str = \", \".join(overlap)\n",
    "    print(f\"Overlapping players: {players_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analytical_report",
   "metadata": {},
   "source": [
    "## Analytical Report Summary\n",
    "\n",
    "### Data Engineering Questions - Findings:\n",
    "\n",
    "#### Question 1: Position Analysis\n",
    "**Finding:** The analysis reveals which positions score the highest average points across all seasons.\n",
    "\n",
    "**Key Insights:**\n",
    "- Midfielders and Forwards typically score higher average points due to goals/assists\n",
    "- Defenders score more consistently (clean sheets) but with lower ceilings\n",
    "- Goalkeepers have the most consistent but lowest average points\n",
    "- Box plot shows forwards have highest variance (boom or bust performances)\n",
    "\n",
    "**Implications for modeling:**\n",
    "- Position must be included as a feature (encoded categorically)\n",
    "- Different positions require different evaluation criteria\n",
    "- Defender predictions should emphasize clean_sheets, forwards should emphasize goals_scored\n",
    "\n",
    "#### Question 2: Performance Evolution (2022-23 Season)\n",
    "**Finding:** Top performers by total points vs. top by form metric show interesting patterns.\n",
    "\n",
    "**Key Insights:**\n",
    "- Players with highest total points aren't always \"in form\" every week\n",
    "- Form metric (4-week rolling average) captures hot/cold streaks\n",
    "- Consistency matters: Some players score fewer total points but maintain steady form\n",
    "- Form trends show momentum: Good form often continues for 2-3 gameweeks\n",
    "\n",
    "**Implications for modeling:**\n",
    "- Form feature is valuable for capturing recent performance trends\n",
    "- Historical total points alone insufficient - recent form better predicts next week\n",
    "- Model should weight recent performance (form) alongside current gameweek stats\n",
    "\n",
    "### Feature Engineering Decisions:\n",
    "\n",
    "**Form Feature Creation:**\n",
    "- 4-week window chosen to balance recency vs. statistical stability\n",
    "- Divided by 10 to normalize scale (typical FPL points per gameweek: 0-15)\n",
    "- Rolling calculation respects player-season boundaries\n",
    "- Captures momentum without being too reactive to single-game anomalies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modeling_header",
   "metadata": {},
   "source": [
    "## 6. Predictive Modeling\n",
    "\n",
    "### Create Target Variable: upcoming_total_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_target",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create upcoming_total_points\n",
    "def create_upcoming_points(group):\n",
    "    group['upcoming_total_points'] = group['total_points'].shift(-1)\n",
    "    return group\n",
    "\n",
    "df_model = df_clean.groupby(['season', 'player_name'], group_keys=False).apply(create_upcoming_points)\n",
    "df_model = df_model.dropna(subset=['upcoming_total_points'])\n",
    "\n",
    "print(f'Dataset shape after creating target: {df_model.shape}')\n",
    "print(f'\\nSample data:')\n",
    "df_model[['player_name', 'gameweek', 'total_points', 'upcoming_total_points']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_selection_header",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "**Input Features:**\n",
    "- Match-related: goals_scored, assists, minutes, clean_sheets\n",
    "- Player-related: position, creativity, influence, value\n",
    "- Engineered: form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_justification",
   "metadata": {},
   "source": [
    "### Feature Selection Justification\n",
    "\n",
    "**Why these features were selected:**\n",
    "\n",
    "#### Match Performance Features:\n",
    "1. **goals_scored**: Direct indicator of offensive contribution. In FPL scoring, goals are heavily weighted (4-6 points depending on position).\n",
    "2. **assists**: Measures playmaking ability. Each assist awards 3 points in FPL.\n",
    "3. **minutes**: Playing time is fundamental - players must be on the pitch to score points. Strong predictor of availability and fitness.\n",
    "4. **clean_sheets**: Critical for defenders and goalkeepers (4-6 points). Indicates defensive solidity.\n",
    "\n",
    "#### Player Quality Metrics:\n",
    "5. **creativity**: FPL metric measuring chance creation quality. Players with high creativity consistently contribute to goal-scoring opportunities.\n",
    "6. **influence**: Captures overall impact on match outcomes. High-influence players are involved in key moments.\n",
    "7. **value**: Player cost reflects expected performance. More expensive players typically deliver higher returns.\n",
    "\n",
    "#### Engineered Features:\n",
    "8. **form**: Rolling 4-week average captures recent performance trends. Players in good form tend to maintain momentum.\n",
    "9. **position**: Different positions have different scoring patterns (forwards score more goals, defenders get clean sheet bonuses).\n",
    "\n",
    "**Why these features work together:**\n",
    "- **Recent performance** (form) + **current gameweek stats** = Strong predictor of next week\n",
    "- **Quality metrics** (creativity, influence) provide context beyond raw statistics\n",
    "- **Position** accounts for role-based scoring differences\n",
    "\n",
    "**Features excluded:**\n",
    "- Popularity metrics (transfers_in, selected_by_percent): These reflect manager decisions, not player performance\n",
    "- Match context (opponent, home/away): These add complexity without proportional predictive gain for this model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "feature_cols = [\n",
    "    'goals_scored', 'assists', 'minutes', 'clean_sheets',\n",
    "    'position', 'creativity', 'influence', 'value', 'form'\n",
    "]\n",
    "\n",
    "target_col = 'upcoming_total_points'\n",
    "\n",
    "X = df_model[feature_cols].copy()\n",
    "y = df_model[target_col].copy()\n",
    "\n",
    "# Encode position\n",
    "le = LabelEncoder()\n",
    "X['position_encoded'] = le.fit_transform(X['position'])\n",
    "X = X.drop('position', axis=1)\n",
    "\n",
    "print(f'Feature matrix shape: {X.shape}')\n",
    "print(f'Target variable shape: {y.shape}')\n",
    "print(f'\\nFeatures used: {X.columns.tolist()}')\n",
    "print(f'\\nFeature correlations with target:')\n",
    "correlations = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Correlation': [X[col].corr(y) for col in X.columns]\n",
    "}).sort_values('Correlation', ascending=False)\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train_split_header",
   "metadata": {},
   "source": [
    "### Train-Test Split and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_test_split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'Training set size: {X_train.shape[0]}')\n",
    "print(f'Test set size: {X_test.shape[0]}')\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print('\\nFeatures scaled successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_training_header",
   "metadata": {},
   "source": [
    "### Model Training - Ridge Regression\n",
    "\n",
    "We use Ridge Regression (regularized linear regression) to predict upcoming player points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_choice_justification",
   "metadata": {},
   "source": [
    "### Why Ridge Regression?\n",
    "\n",
    "**Statistical ML Model Choice:**\n",
    "\n",
    "**1. Regularization Benefits:**\n",
    "- Ridge adds L2 penalty to prevent overfitting\n",
    "- Handles multicollinearity between features (e.g., creativity and influence are correlated)\n",
    "- Shrinks coefficients of less important features without eliminating them\n",
    "\n",
    "**2. Interpretability:**\n",
    "- Coefficients directly show feature importance and direction of impact\n",
    "- Linear relationship is intuitive: \"Each additional goal increases predicted points by X\"\n",
    "- Easier to explain to stakeholders compared to black-box models\n",
    "\n",
    "**3. Computational Efficiency:**\n",
    "- Fast training on large datasets (multiple seasons of player data)\n",
    "- Quick inference for real-time predictions\n",
    "- Scales well with additional features\n",
    "\n",
    "**4. Regression Problem Nature:**\n",
    "- FPL points are continuous values (0-20+ range)\n",
    "- Linear relationships exist: more goals → more points, more minutes → more points\n",
    "- Ridge captures these linear trends while handling noise\n",
    "\n",
    "**Alpha = 1.0 choice:**\n",
    "- Moderate regularization strength\n",
    "- Balances model complexity with predictive power\n",
    "- Can be tuned via cross-validation for optimal performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Ridge Regression model\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "print(\"Training Ridge Regression Model...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize Ridge Regression with alpha=1.0 (regularization strength)\n",
    "model = Ridge(alpha=1.0, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = model.predict(X_train_scaled)\n",
    "y_pred_test = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate on training set\n",
    "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"\\nTraining Set Performance:\")\n",
    "print(f\"  MAE:  {train_mae:.4f}\")\n",
    "print(f\"  MSE:  {train_mse:.4f}\")\n",
    "print(f\"  RMSE: {train_rmse:.4f}\")\n",
    "print(f\"  R²:   {train_r2:.4f}\")\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(f\"  MAE:  {test_mae:.4f}\")\n",
    "print(f\"  MSE:  {test_mse:.4f}\")\n",
    "print(f\"  RMSE: {test_rmse:.4f}\")\n",
    "print(f\"  R²:   {test_r2:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Model training completed successfully!\")\n",
    "\n",
    "# Store results for visualization\n",
    "results = {\n",
    "    \"MAE\": test_mae,\n",
    "    \"MSE\": test_mse,\n",
    "    \"RMSE\": test_rmse,\n",
    "    \"R²\": test_r2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_viz_header",
   "metadata": {},
   "source": [
    "### Model Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance metrics\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Metrics comparison\n",
    "metrics = list(results.keys())\n",
    "values = list(results.values())\n",
    "\n",
    "axes[0].bar(metrics, values, color=[\"#FF6B6B\", \"#4ECDC4\", \"#45B7D1\", \"#FFA07A\"])\n",
    "axes[0].set_title(\"Ridge Regression - Performance Metrics\", fontsize=14, fontweight=\"bold\")\n",
    "axes[0].set_ylabel(\"Metric Value\", fontsize=12)\n",
    "axes[0].set_xlabel(\"Metric\", fontsize=12)\n",
    "axes[0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(values):\n",
    "    axes[0].text(i, v + 0.1, f\"{v:.4f}\", ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "\n",
    "# Plot 2: Actual vs Predicted\n",
    "axes[1].scatter(y_test, y_pred_test, alpha=0.5, s=10, color=\"#45B7D1\")\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"r--\", lw=2, label=\"Perfect Prediction\")\n",
    "axes[1].set_xlabel(\"Actual Points\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Predicted Points\", fontsize=12)\n",
    "axes[1].set_title(\"Actual vs Predicted Points - Ridge Regression\", fontsize=14, fontweight=\"bold\")\n",
    "axes[1].legend(loc=\"best\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional visualization: Residuals plot\n",
    "residuals = y_test - y_pred_test\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Residuals scatter plot\n",
    "axes[0].scatter(y_pred_test, residuals, alpha=0.5, s=10, color=\"#FF6B6B\")\n",
    "axes[0].axhline(y=0, color=\"black\", linestyle=\"--\", lw=2)\n",
    "axes[0].set_xlabel(\"Predicted Points\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Residuals\", fontsize=12)\n",
    "axes[0].set_title(\"Residual Plot\", fontsize=14, fontweight=\"bold\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals histogram\n",
    "axes[1].hist(residuals, bins=50, color=\"#4ECDC4\", edgecolor=\"black\", alpha=0.7)\n",
    "axes[1].set_xlabel(\"Residuals\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Frequency\", fontsize=12)\n",
    "axes[1].set_title(\"Distribution of Residuals\", fontsize=14, fontweight=\"bold\")\n",
    "axes[1].axvline(x=0, color=\"red\", linestyle=\"--\", lw=2)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results_interpretation",
   "metadata": {},
   "source": [
    "### Model Results Interpretation\n",
    "\n",
    "**Understanding the Metrics:**\n",
    "\n",
    "**Mean Absolute Error (MAE):**\n",
    "- Average absolute difference between predicted and actual points\n",
    "- Interpretation: On average, predictions are off by ~MAE points\n",
    "- Lower is better. In FPL context, MAE < 2.5 is reasonable given point variance\n",
    "\n",
    "**Root Mean Squared Error (RMSE):**\n",
    "- Penalizes larger errors more heavily than MAE\n",
    "- RMSE > MAE indicates some predictions have large errors (expected due to unpredictable events like red cards, penalties)\n",
    "- Measures model's typical prediction error magnitude\n",
    "\n",
    "**R² (R-squared):**\n",
    "- Proportion of variance in points explained by the model\n",
    "- Range: 0 to 1 (higher is better)\n",
    "- FPL is inherently noisy (injuries, tactical changes, luck), so R² of 0.3-0.5 is reasonable\n",
    "- Shows model captures meaningful patterns despite randomness in football\n",
    "\n",
    "**Why not 100% accuracy?**\n",
    "- Football has inherent randomness: injuries, referee decisions, weather, opponent tactics\n",
    "- Unexpected events (e.g., goalkeeper scoring a goal) are unpredictable\n",
    "- Model predicts expected performance, not guaranteed outcomes\n",
    "- Our goal: Beat random guessing and capture systematic patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shap_header",
   "metadata": {},
   "source": [
    "## 7. Model Explainability - SHAP\n",
    "\n",
    "Using SHAP to understand feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shap_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP explainer for Ridge Regression\n",
    "print(\"Creating SHAP explainer...\")\n",
    "\n",
    "sample_size = min(1000, len(X_test_scaled))\n",
    "X_sample = X_test_scaled[:sample_size]\n",
    "\n",
    "# Use Linear explainer for Ridge Regression\n",
    "explainer = shap.LinearExplainer(model, X_train_scaled)\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "print(\"SHAP values calculated successfully!\")\n",
    "\n",
    "# Summary plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_sample, feature_names=X.columns.tolist(), show=False)\n",
    "plt.title(\"SHAP Summary Plot - Feature Importance\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "shap.summary_plot(shap_values, X_sample, feature_names=X.columns.tolist(), \n",
    "                 plot_type=\"bar\", show=False)\n",
    "plt.title(\"SHAP Feature Importance - Bar Plot\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance from Ridge coefficients\n",
    "feature_importance = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Coefficient\": model.coef_,\n",
    "    \"Abs_Coefficient\": np.abs(model.coef_)\n",
    "}).sort_values(\"Abs_Coefficient\", ascending=False)\n",
    "\n",
    "print(\"\\nRidge Regression Feature Coefficients:\")\n",
    "print(feature_importance)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(feature_importance[\"Feature\"], feature_importance[\"Coefficient\"], \n",
    "         color=[\"#FF6B6B\" if x < 0 else \"#4ECDC4\" for x in feature_importance[\"Coefficient\"]])\n",
    "plt.xlabel(\"Coefficient Value\", fontsize=12)\n",
    "plt.title(\"Ridge Regression - Feature Coefficients\", fontsize=14, fontweight=\"bold\")\n",
    "plt.grid(axis=\"x\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lime_header",
   "metadata": {},
   "source": [
    "## 8. Model Explainability - LIME\n",
    "\n",
    "Using LIME to explain individual predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lime_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LIME explainer\n",
    "print(\"Creating LIME explainer...\")\n",
    "\n",
    "lime_explainer = lime_tabular.LimeTabularExplainer(\n",
    "    training_data=X_train_scaled,\n",
    "    feature_names=X.columns.tolist(),\n",
    "    mode=\"regression\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Explain predictions\n",
    "num_explanations = 3\n",
    "indices_to_explain = np.random.choice(len(X_test_scaled), num_explanations, replace=False)\n",
    "\n",
    "for i, idx in enumerate(indices_to_explain):\n",
    "    print(f\"\\nExplaining prediction {i+1}:\")\n",
    "    \n",
    "    explanation = lime_explainer.explain_instance(\n",
    "        data_row=X_test_scaled[idx],\n",
    "        predict_fn=model.predict,\n",
    "        num_features=len(X.columns)\n",
    "    )\n",
    "    \n",
    "    actual = y_test.iloc[idx]\n",
    "    predicted = model.predict(X_test_scaled[idx].reshape(1, -1))[0]\n",
    "    print(f\"Actual: {actual:.2f}, Predicted: {predicted:.2f}\")\n",
    "    \n",
    "    fig = explanation.as_pyplot_figure()\n",
    "    plt.title(f\"LIME Explanation {i+1} - Actual: {actual:.2f}, Predicted: {predicted:.2f}\", \n",
    "              fontsize=12, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nLIME analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference_header",
   "metadata": {},
   "source": [
    "## 9. Inference Function\n",
    "\n",
    "Create a callable function for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_upcoming_points(player_data):\n",
    "    \"\"\"\n",
    "    Predict upcoming total points for a player using Ridge Regression.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    player_data : dict or pd.DataFrame\n",
    "        Player statistics containing:\n",
    "        - goals_scored, assists, minutes, clean_sheets\n",
    "        - position (\"GK\", \"DEF\", \"MID\", \"FWD\")\n",
    "        - creativity, influence, value, form\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float: Predicted upcoming total points\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(player_data, dict):\n",
    "        player_data = pd.DataFrame([player_data])\n",
    "    \n",
    "    if \"form\" not in player_data.columns:\n",
    "        player_data[\"form\"] = 0.0\n",
    "    \n",
    "    features = [\"goals_scored\", \"assists\", \"minutes\", \"clean_sheets\", \n",
    "                \"creativity\", \"influence\", \"value\", \"form\"]\n",
    "    \n",
    "    X_input = player_data[features].copy()\n",
    "    position_encoded = le.transform(player_data[\"position\"])[0]\n",
    "    X_input[\"position_encoded\"] = position_encoded\n",
    "    \n",
    "    X_scaled = scaler.transform(X_input)\n",
    "    prediction = model.predict(X_scaled)[0]\n",
    "    \n",
    "    return round(prediction, 2)\n",
    "\n",
    "\n",
    "# Test the function\n",
    "print(\"Testing Inference Function:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_player_1 = {\n",
    "    \"goals_scored\": 2, \"assists\": 1, \"minutes\": 90, \"clean_sheets\": 0,\n",
    "    \"position\": \"MID\", \"creativity\": 80.0, \"influence\": 75.0, \n",
    "    \"value\": 100.0, \"form\": 0.8\n",
    "}\n",
    "\n",
    "prediction_1 = predict_upcoming_points(test_player_1)\n",
    "print(f\"\\nExample 1 - High-performing Midfielder:\")\n",
    "print(f\"Input: {test_player_1}\")\n",
    "print(f\"Predicted upcoming points: {prediction_1}\")\n",
    "\n",
    "test_player_2 = {\n",
    "    \"goals_scored\": 0, \"assists\": 0, \"minutes\": 90, \"clean_sheets\": 1,\n",
    "    \"position\": \"DEF\", \"creativity\": 30.0, \"influence\": 50.0, \n",
    "    \"value\": 50.0, \"form\": 0.5\n",
    "}\n",
    "\n",
    "prediction_2 = predict_upcoming_points(test_player_2)\n",
    "print(f\"\\nExample 2 - Solid Defender:\")\n",
    "print(f\"Input: {test_player_2}\")\n",
    "print(f\"Predicted upcoming points: {prediction_2}\")\n",
    "\n",
    "test_player_3 = {\n",
    "    \"goals_scored\": 1, \"assists\": 0, \"minutes\": 90, \"clean_sheets\": 0,\n",
    "    \"position\": \"FWD\", \"creativity\": 50.0, \"influence\": 60.0, \n",
    "    \"value\": 90.0, \"form\": 0.6\n",
    "}\n",
    "\n",
    "prediction_3 = predict_upcoming_points(test_player_3)\n",
    "print(f\"\\nExample 3 - Forward with 1 Goal:\")\n",
    "print(f\"Input: {test_player_3}\")\n",
    "print(f\"Predicted upcoming points: {prediction_3}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Inference function created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_model_header",
   "metadata": {},
   "source": [
    "## 10. Save Model and Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_artifacts",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save Ridge Regression model\n",
    "with open(\"fpl_ridge_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# Save scaler\n",
    "with open(\"fpl_scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Save label encoder\n",
    "with open(\"fpl_label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "# Save cleaned dataset\n",
    "df_model.to_csv(\"fpl_cleaned_with_features.csv\", index=False)\n",
    "\n",
    "print(\"Model and artifacts saved successfully!\")\n",
    "print(\"\\nSaved files:\")\n",
    "print(\"- fpl_ridge_model.pkl (Ridge Regression model)\")\n",
    "print(\"- fpl_scaler.pkl (Feature scaler)\")\n",
    "print(\"- fpl_label_encoder.pkl (Position encoder)\")\n",
    "print(\"- fpl_cleaned_with_features.csv (Dataset with all features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_header",
   "metadata": {},
   "source": [
    "## 11. Project Summary\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Data Cleaning**: Successfully cleaned dataset\n",
    "2. **Feature Engineering**: Created 'form' feature\n",
    "3. **Position Analysis**: Identified highest scoring positions\n",
    "4. **Performance Evolution**: Tracked top 5 players in 2022-23\n",
    "5. **Predictive Model**: Compared multiple regression models\n",
    "6. **Model Explainability**: SHAP and LIME analysis completed\n",
    "7. **Inference Function**: Production-ready prediction function\n",
    "\n",
    "### Deliverables Completed:\n",
    "✓ Jupyter Notebook with full workflow\n",
    "✓ Cleaned dataset with 'form' column\n",
    "✓ Analytical report with visualizations\n",
    "✓ Regression model for upcoming_total_points\n",
    "✓ SHAP and LIME explainability outputs\n",
    "✓ Inference function for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final summary\n",
    "print(\"PROJECT COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"- Total records processed: {len(df_clean):,}\")\n",
    "print(f\"- Total records for modeling: {len(df_model):,}\")\n",
    "print(f\"- Number of unique players: {df_clean['player_name'].nunique():,}\")\n",
    "print(f\"- Seasons covered: {df_clean['season'].unique().tolist()}\")\n",
    "print(f\"\\nModel: Ridge Regression (Regularized Linear Regression)\")\n",
    "print(f\"Model Performance on Test Set:\")\n",
    "print(f\"- MAE:  {results['MAE']:.4f}\")\n",
    "print(f\"- MSE:  {results['MSE']:.4f}\")\n",
    "print(f\"- RMSE: {results['RMSE']:.4f}\")\n",
    "print(f\"- R²:   {results['R²']:.4f}\")\n",
    "print(\"\\n\" + \"=\"*70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
